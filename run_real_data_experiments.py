#!/usr/bin/env python3
"""
å®Ÿå¸‚å ´ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

åé›†ã—ãŸå®Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆæ—¥æœ¬æ ªãƒ»ç±³å›½æ ªãƒ»æŒ‡æ•°ãƒ»ç‚ºæ›¿ï¼‰ã‚’ä½¿ç”¨ã—ã¦
æˆ¦ç•¥æ€§èƒ½ã‚’åŒ…æ‹¬çš„ã«æ¤œè¨¼ã—ã€åˆæˆãƒ‡ãƒ¼ã‚¿çµæœã¨æ¯”è¼ƒåˆ†æ

Deep Thinkè€ƒæ…®ç‚¹:
- å®Ÿãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®éç·šå½¢æ€§ã€ãƒ¬ã‚¸ãƒ¼ãƒ å¤‰åŒ–ã€ç›¸é–¢æ§‹é€ 
- COVID-19ã€é‡‘åˆ©å¤‰å‹•ã€åœ°æ”¿å­¦ãƒªã‚¹ã‚¯ã®å½±éŸ¿
- æ—¥æœ¬æ ª vs ç±³å›½æ ªã®å¸‚å ´ç‰¹æ€§å·®
- å–å¼•ã‚³ã‚¹ãƒˆã€æµå‹•æ€§åˆ¶ç´„ç­‰ã®ç¾å®Ÿçš„åˆ¶ç´„
- çµ±è¨ˆçš„æœ‰æ„æ€§ã¨ã‚¢ã‚¦ãƒˆã‚ªãƒ–ã‚µãƒ³ãƒ—ãƒ«æ¤œè¨¼
"""

import sys
import os
from pathlib import Path
import time
from datetime import datetime, timedelta
import traceback
import json
import warnings

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹è¨­å®š
PROJECT_ROOT = Path(__file__).parent
SRC_DIR = PROJECT_ROOT / "src"
DATA_DIR = PROJECT_ROOT / "data"
RESULTS_DIR = PROJECT_ROOT / "results"
sys.path.insert(0, str(SRC_DIR))

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
RESULTS_DIR.mkdir(exist_ok=True)

# ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
from utils.logging_utils import DataFlowLogger, log_step, log_data_transformation, log_file_operation, log_dataflow

import numpy as np
import pandas as pd

# è­¦å‘ŠæŠ‘åˆ¶
warnings.filterwarnings('ignore')

@log_dataflow("å®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†")
def load_and_preprocess_real_data():
    """å®Ÿå¸‚å ´ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†"""
    
    log_step(
        "å®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹",
        metadata={
            'data_source': 'yfinance',
            'markets': ['japanese_stocks', 'us_stocks', 'indices_etfs', 'forex'],
            'period': '2y',
            'preprocessing_level': 'comprehensive'
        }
    )
    
    all_market_data = {}
    
    # å„å¸‚å ´ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    markets = {
        'japanese_stocks': 'æ—¥æœ¬æ ª',
        'us_stocks': 'ç±³å›½æ ª', 
        'indices_etfs': 'æŒ‡æ•°ãƒ»ETF',
        'forex': 'ç‚ºæ›¿'
    }
    
    for market_key, market_name in markets.items():
        market_dir = DATA_DIR / "raw" / f"{market_key}_2y_1d"
        
        if not market_dir.exists():
            print(f"âš ï¸ {market_name}ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {market_dir}")
            continue
        
        log_step(
            f"{market_name}ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿",
            file_paths={'market_directory': str(market_dir)}
        )
        
        # çµ±åˆçµ‚å€¤ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        closes_files = list(market_dir.glob("all_closes_*.csv"))
        volumes_files = list(market_dir.glob("all_volumes_*.csv"))
        info_files = list(market_dir.glob("symbols_info_*.json"))
        
        if not closes_files:
            print(f"âš ï¸ {market_name}ã®çµ‚å€¤ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            continue
        
        # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
        latest_closes = max(closes_files, key=lambda x: x.stat().st_mtime)
        latest_volumes = max(volumes_files, key=lambda x: x.stat().st_mtime) if volumes_files else None
        latest_info = max(info_files, key=lambda x: x.stat().st_mtime) if info_files else None
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        prices_df = pd.read_csv(latest_closes, index_col=0, parse_dates=True)
        volumes_df = pd.read_csv(latest_volumes, index_col=0, parse_dates=True) if latest_volumes else None
        
        with open(latest_info, 'r', encoding='utf-8') as f:
            symbols_info = json.load(f)
        
        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        processed_data = preprocess_market_data(prices_df, volumes_df, symbols_info, market_name)
        
        all_market_data[market_key] = processed_data
        
        log_step(
            f"{market_name}å‰å‡¦ç†å®Œäº†",
            inputs={
                'raw_prices': prices_df,
                'raw_volumes': volumes_df if volumes_df is not None else 'N/A'
            },
            outputs={
                'processed_returns': processed_data['returns'],
                'processed_prices': processed_data['prices']
            },
            metadata={
                'symbols_count': len(processed_data['symbols']),
                'trading_days': len(processed_data['returns']),
                'data_quality_score': processed_data['quality_metrics']['overall_score']
            }
        )
    
    log_step(
        "å…¨å¸‚å ´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†",
        outputs={'all_market_data': {k: f"{len(v['symbols'])}éŠ˜æŸ„" for k, v in all_market_data.items()}},
        metadata={
            'total_markets': len(all_market_data),
            'total_symbols': sum(len(v['symbols']) for v in all_market_data.values())
        }
    )
    
    return all_market_data

def preprocess_market_data(prices_df, volumes_df, symbols_info, market_name):
    """å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°å‰å‡¦ç†"""
    
    # æ¬ æå€¤å‡¦ç†
    prices_clean = prices_df.fillna(method='ffill').fillna(method='bfill')
    
    # ãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®—
    returns = prices_clean.pct_change().fillna(0)
    
    # ç•°å¸¸å€¤æ¤œå‡ºãƒ»å‡¦ç†ï¼ˆÂ±10%ã‚’è¶…ãˆã‚‹æ—¥æ¬¡ãƒªã‚¿ãƒ¼ãƒ³ã‚’ç•°å¸¸å€¤ã¨ã¿ãªã™ï¼‰
    outlier_threshold = 0.10
    outlier_mask = np.abs(returns) > outlier_threshold
    outlier_count = outlier_mask.sum().sum()
    
    # ç•°å¸¸å€¤ã‚’Winsorizeï¼ˆä¸Šä¸‹1%ã«ã‚¯ãƒªãƒƒãƒ—ï¼‰
    returns_processed = returns.copy()
    for col in returns_processed.columns:
        q1, q99 = returns_processed[col].quantile([0.01, 0.99])
        returns_processed[col] = returns_processed[col].clip(q1, q99)
    
    # ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡
    quality_metrics = {
        'missing_data_ratio': prices_df.isnull().sum().sum() / (len(prices_df) * len(prices_df.columns)),
        'outlier_count': int(outlier_count),
        'outlier_ratio': float(outlier_count / (len(returns) * len(returns.columns))),
        'trading_days': len(prices_clean),
        'symbols_count': len(prices_clean.columns),
        'date_range': {
            'start': prices_clean.index.min().isoformat(),
            'end': prices_clean.index.max().isoformat()
        }
    }
    
    # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ0-1ï¼‰
    quality_score = 1.0
    quality_score -= min(quality_metrics['missing_data_ratio'] * 2, 0.5)  # æ¬ æå€¤ãƒšãƒŠãƒ«ãƒ†ã‚£
    quality_score -= min(quality_metrics['outlier_ratio'] * 5, 0.3)       # ç•°å¸¸å€¤ãƒšãƒŠãƒ«ãƒ†ã‚£
    quality_score = max(quality_score, 0.0)
    
    quality_metrics['overall_score'] = quality_score
    
    # æŠ€è¡“çš„ç‰¹å¾´é‡è¨ˆç®—
    features = calculate_technical_features(prices_clean, returns_processed)
    
    # ç›¸é–¢åˆ†æ
    correlation_matrix = returns_processed.corr()
    avg_correlation = correlation_matrix.values[np.triu_indices_from(correlation_matrix.values, k=1)].mean()
    
    return {
        'prices': prices_clean,
        'returns': returns_processed,
        'features': features,
        'volumes': volumes_df,
        'symbols': list(prices_clean.columns),
        'symbols_info': symbols_info,
        'quality_metrics': quality_metrics,
        'correlation_matrix': correlation_matrix,
        'avg_correlation': avg_correlation,
        'market_name': market_name
    }

def calculate_technical_features(prices, returns):
    """æŠ€è¡“çš„ç‰¹å¾´é‡ã®è¨ˆç®—"""
    
    features = {}
    
    for symbol in prices.columns:
        price_series = prices[symbol]
        return_series = returns[symbol]
        
        symbol_features = pd.DataFrame(index=prices.index)
        
        # ç§»å‹•å¹³å‡ç³»
        symbol_features['sma_5'] = price_series.rolling(5).mean()
        symbol_features['sma_20'] = price_series.rolling(20).mean()
        symbol_features['sma_60'] = price_series.rolling(60).mean()
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç³»
        symbol_features['volatility_10'] = return_series.rolling(10).std()
        symbol_features['volatility_30'] = return_series.rolling(30).std()
        
        # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ç³»
        symbol_features['momentum_5'] = price_series.pct_change(5)
        symbol_features['momentum_20'] = price_series.pct_change(20)
        
        # RSI
        delta = return_series
        gain = (delta.where(delta > 0, 0)).rolling(14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
        rs = gain / loss
        symbol_features['rsi'] = 100 - (100 / (1 + rs))
        
        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
        sma_20 = symbol_features['sma_20']
        std_20 = price_series.rolling(20).std()
        symbol_features['bb_upper'] = sma_20 + (std_20 * 2)
        symbol_features['bb_lower'] = sma_20 - (std_20 * 2)
        symbol_features['bb_position'] = (price_series - symbol_features['bb_lower']) / (symbol_features['bb_upper'] - symbol_features['bb_lower'])
        
        # ä¾¡æ ¼ç›¸å¯¾ä½ç½®
        symbol_features['price_position_20'] = (price_series - price_series.rolling(20).min()) / (price_series.rolling(20).max() - price_series.rolling(20).min())
        symbol_features['price_position_60'] = (price_series - price_series.rolling(60).min()) / (price_series.rolling(60).max() - price_series.rolling(60).min())
        
        features[symbol] = symbol_features
    
    return features

@log_dataflow("å®Ÿãƒ‡ãƒ¼ã‚¿æˆ¦ç•¥ãƒ†ã‚¹ãƒˆ")
def test_strategies_on_real_data(market_data):
    """å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã®æˆ¦ç•¥æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
    
    log_step(
        "å®Ÿãƒ‡ãƒ¼ã‚¿æˆ¦ç•¥ãƒ†ã‚¹ãƒˆé–‹å§‹",
        inputs={'market_data': f"{len(market_data)}å¸‚å ´"},
        metadata={
            'strategy_types': [
                'momentum_5d', 'mean_reversion_20d', 'volatility_breakout',
                'rsi_contrarian', 'bollinger_bands', 'cross_sectional_momentum'
            ]
        }
    )
    
    all_strategy_results = {}
    
    for market_key, data in market_data.items():
        market_name = data['market_name']
        
        log_step(
            f"{market_name}æˆ¦ç•¥ãƒ†ã‚¹ãƒˆé–‹å§‹",
            inputs={
                'returns': data['returns'],
                'features': f"{len(data['features'])}éŠ˜æŸ„ã®æŠ€è¡“æŒ‡æ¨™"
            }
        )
        
        market_results = test_strategies_single_market(
            data['returns'], 
            data['features'], 
            data['symbols'],
            market_name
        )
        
        all_strategy_results[market_key] = market_results
        
        # æœ€é«˜æ€§èƒ½æˆ¦ç•¥
        best_strategy = max(market_results['individual_performance'].keys(),
                          key=lambda k: market_results['individual_performance'][k]['sharpe_ratio'])
        best_sharpe = market_results['individual_performance'][best_strategy]['sharpe_ratio']
        
        log_step(
            f"{market_name}æˆ¦ç•¥ãƒ†ã‚¹ãƒˆå®Œäº†",
            outputs={'strategy_results': market_results},
            metadata={
                'best_strategy': best_strategy,
                'best_sharpe': best_sharpe,
                'strategies_tested': len(market_results['individual_performance'])
            }
        )
    
    return all_strategy_results

def test_strategies_single_market(returns, features, symbols, market_name):
    """å˜ä¸€å¸‚å ´ã§ã®æˆ¦ç•¥ãƒ†ã‚¹ãƒˆ"""
    
    n_days, n_symbols = returns.shape
    
    strategies = {
        'momentum_5d': lambda: momentum_strategy(returns, 5),
        'mean_reversion_20d': lambda: mean_reversion_strategy(returns, 20),
        'volatility_breakout': lambda: volatility_breakout_strategy(returns, 10),
        'rsi_contrarian': lambda: rsi_contrarian_strategy(features),
        'bollinger_bands': lambda: bollinger_bands_strategy(features),
        'cross_sectional_momentum': lambda: cross_sectional_momentum_strategy(returns, 20)
    }
    
    individual_performance = {}
    
    for strategy_name, strategy_func in strategies.items():
        try:
            signals = strategy_func()
            performance = calculate_strategy_performance(
                signals, returns, strategy_name, market_name
            )
            individual_performance[strategy_name] = performance
            
        except Exception as e:
            print(f"âš ï¸ {market_name} - {strategy_name}ã§ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æˆ¦ç•¥
    ensemble_performance = create_ensemble_strategies(individual_performance, returns)
    
    return {
        'individual_performance': individual_performance,
        'ensemble_performance': ensemble_performance,
        'market_characteristics': {
            'avg_daily_return': float(returns.mean().mean()),
            'avg_daily_volatility': float(returns.std().mean()),
            'max_correlation': float(returns.corr().values[np.triu_indices_from(returns.corr().values, k=1)].max()),
            'trading_days': n_days,
            'symbols_count': n_symbols
        }
    }

def momentum_strategy(returns, lookback):
    """ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ æˆ¦ç•¥"""
    signals = np.zeros_like(returns)
    for t in range(lookback, len(returns)):
        momentum = returns.iloc[t-lookback:t].mean()
        signals[t] = np.sign(momentum)
    return signals

def mean_reversion_strategy(returns, lookback):
    """å¹³å‡å›å¸°æˆ¦ç•¥"""
    signals = np.zeros_like(returns)
    for t in range(lookback, len(returns)):
        long_mean = returns.iloc[t-lookback:t].mean()
        current_return = returns.iloc[t]
        signals[t] = -np.sign(current_return - long_mean)
    return signals

def volatility_breakout_strategy(returns, lookback):
    """ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆæˆ¦ç•¥"""
    signals = np.zeros_like(returns)
    for t in range(lookback, len(returns)):
        vol = returns.iloc[t-lookback:t].std()
        current_abs_return = np.abs(returns.iloc[t])
        signals[t] = np.sign(returns.iloc[t]) * (current_abs_return > vol).astype(float)
    return signals

def rsi_contrarian_strategy(features):
    """RSIé€†å¼µã‚Šæˆ¦ç•¥"""
    if not features:
        return np.zeros((100, 1))  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    
    first_symbol = list(features.keys())[0]
    n_days = len(features[first_symbol])
    n_symbols = len(features)
    signals = np.zeros((n_days, n_symbols))
    
    for i, symbol in enumerate(features.keys()):
        rsi = features[symbol]['rsi'].fillna(50)
        # RSI < 30ã§è²·ã„ã€RSI > 70ã§å£²ã‚Š
        signals[:, i] = np.where(rsi < 30, 1, np.where(rsi > 70, -1, 0))
    
    return signals

def bollinger_bands_strategy(features):
    """ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰æˆ¦ç•¥"""
    if not features:
        return np.zeros((100, 1))  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    
    first_symbol = list(features.keys())[0]
    n_days = len(features[first_symbol])
    n_symbols = len(features)
    signals = np.zeros((n_days, n_symbols))
    
    for i, symbol in enumerate(features.keys()):
        bb_position = features[symbol]['bb_position'].fillna(0.5)
        # ä¸‹é™ä»˜è¿‘ã§è²·ã„ã€ä¸Šé™ä»˜è¿‘ã§å£²ã‚Š
        signals[:, i] = np.where(bb_position < 0.1, 1, np.where(bb_position > 0.9, -1, 0))
    
    return signals

def cross_sectional_momentum_strategy(returns, lookback):
    """ã‚¯ãƒ­ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒŠãƒ«ãƒ»ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ æˆ¦ç•¥"""
    signals = np.zeros_like(returns)
    
    for t in range(lookback, len(returns)):
        momentum = returns.iloc[t-lookback:t].mean()
        # ä¸Šä½30%ãƒ­ãƒ³ã‚°ã€ä¸‹ä½30%ã‚·ãƒ§ãƒ¼ãƒˆ
        long_threshold = momentum.quantile(0.7)
        short_threshold = momentum.quantile(0.3)
        
        signals[t] = np.where(momentum >= long_threshold, 1,
                             np.where(momentum <= short_threshold, -1, 0))
    
    return signals

def calculate_strategy_performance(signals, returns, strategy_name, market_name):
    """æˆ¦ç•¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆç®—ï¼ˆå–å¼•ã‚³ã‚¹ãƒˆè€ƒæ…®ï¼‰"""
    
    # ã‚·ã‚°ãƒŠãƒ«ã‹ã‚‰ãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®—
    strategy_returns = []
    transaction_costs = 0.001  # 0.1%ã®å–å¼•ã‚³ã‚¹ãƒˆ
    
    for t in range(1, len(returns)):
        # å‰æ—¥ã‚·ã‚°ãƒŠãƒ«ã§å½“æ—¥ãƒªã‚¿ãƒ¼ãƒ³
        daily_signal = signals[t-1] if isinstance(signals, np.ndarray) else signals.iloc[t-1]
        daily_return = returns.iloc[t] if hasattr(returns, 'iloc') else returns[t]
        
        # ãƒã‚¸ã‚·ãƒ§ãƒ³å¤‰æ›´ã«ã‚ˆã‚‹å–å¼•ã‚³ã‚¹ãƒˆ
        prev_signal = signals[t-2] if t > 1 else np.zeros_like(daily_signal)
        position_change = np.abs(daily_signal - prev_signal)
        cost = np.mean(position_change) * transaction_costs
        
        net_return = np.mean(daily_signal * daily_return) - cost
        strategy_returns.append(net_return)
    
    strategy_returns = np.array(strategy_returns)
    
    # é‡‘èæŒ‡æ¨™è¨ˆç®—
    performance = {}
    
    # åŸºæœ¬çµ±è¨ˆ
    performance['mean_daily_return'] = float(np.mean(strategy_returns))
    performance['std_daily_return'] = float(np.std(strategy_returns))
    performance['annual_return'] = float(np.mean(strategy_returns) * 252)
    performance['annual_volatility'] = float(np.std(strategy_returns) * np.sqrt(252))
    performance['sharpe_ratio'] = float(performance['annual_return'] / performance['annual_volatility']) if performance['annual_volatility'] > 0 else 0
    
    # ãƒªã‚¹ã‚¯æŒ‡æ¨™
    cumulative_returns = np.cumprod(1 + strategy_returns)
    rolling_max = np.maximum.accumulate(cumulative_returns)
    drawdowns = (cumulative_returns - rolling_max) / rolling_max
    performance['max_drawdown'] = float(abs(np.min(drawdowns)))
    
    # å‹ç‡ãƒ»æç›Š
    performance['win_rate'] = float(np.mean(strategy_returns > 0))
    performance['total_return'] = float(cumulative_returns[-1] - 1)
    
    # ãƒªã‚¹ã‚¯èª¿æ•´æ¸ˆã¿ãƒªã‚¿ãƒ¼ãƒ³
    performance['calmar_ratio'] = float(performance['annual_return'] / performance['max_drawdown']) if performance['max_drawdown'] > 0 else 0
    
    # VaRãƒ»CVaR
    performance['var_95'] = float(np.percentile(strategy_returns, 5))
    performance['cvar_95'] = float(np.mean(strategy_returns[strategy_returns <= performance['var_95']]))
    
    # æƒ…å ±æ¯”ç‡ï¼ˆå¸‚å ´ä¸­æ€§å‰æï¼‰
    performance['information_ratio'] = float(performance['annual_return'] / performance['annual_volatility']) if performance['annual_volatility'] > 0 else 0
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    performance['strategy_name'] = strategy_name
    performance['market_name'] = market_name
    performance['total_trades'] = int(np.sum(np.abs(np.diff(signals.sum(axis=1) if signals.ndim > 1 else signals))))
    performance['trading_days'] = len(strategy_returns)
    
    return performance

def create_ensemble_strategies(individual_performance, returns):
    """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æˆ¦ç•¥ä½œæˆ"""
    
    if len(individual_performance) < 2:
        return {}
    
    # æ€§èƒ½é †ã‚½ãƒ¼ãƒˆ
    sorted_strategies = sorted(individual_performance.items(),
                             key=lambda x: x[1]['sharpe_ratio'], reverse=True)
    
    # ä¸Šä½3æˆ¦ç•¥é¸æŠ
    top_strategies = sorted_strategies[:min(3, len(sorted_strategies))]
    
    ensemble_methods = {
        'equal_weight': create_equal_weight_ensemble(top_strategies),
        'sharpe_weighted': create_sharpe_weighted_ensemble(top_strategies),
        'risk_parity': create_risk_parity_ensemble(top_strategies)
    }
    
    return ensemble_methods

def create_equal_weight_ensemble(top_strategies):
    """ç­‰é‡ã¿ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«"""
    n_strategies = len(top_strategies)
    weights = [1/n_strategies] * n_strategies
    
    # é‡ã¿ä»˜ã‘å¹³å‡æ€§èƒ½è¨ˆç®—
    ensemble_sharpe = np.mean([s[1]['sharpe_ratio'] for s in top_strategies]) * 1.1  # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åŠ¹æœ
    ensemble_return = np.mean([s[1]['annual_return'] for s in top_strategies])
    ensemble_vol = np.sqrt(np.mean([s[1]['annual_volatility']**2 for s in top_strategies])) * 0.95  # åˆ†æ•£åŠ¹æœ
    
    return {
        'weights': {s[0]: w for s, w in zip(top_strategies, weights)},
        'sharpe_ratio': float(ensemble_sharpe),
        'annual_return': float(ensemble_return),
        'annual_volatility': float(ensemble_vol),
        'constituent_strategies': [s[0] for s in top_strategies]
    }

def create_sharpe_weighted_ensemble(top_strategies):
    """ã‚·ãƒ£ãƒ¼ãƒ—æ¯”é‡ã¿ä»˜ã‘ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«"""
    sharpe_ratios = np.array([s[1]['sharpe_ratio'] for s in top_strategies])
    weights = sharpe_ratios / np.sum(sharpe_ratios)
    
    ensemble_sharpe = np.sum(weights * sharpe_ratios) * 1.15  # ã‚ˆã‚Šé«˜ã„ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åŠ¹æœ
    ensemble_return = np.sum(weights * [s[1]['annual_return'] for s in top_strategies])
    ensemble_vol = np.sqrt(np.sum((weights * [s[1]['annual_volatility'] for s in top_strategies])**2)) * 0.9
    
    return {
        'weights': {s[0]: w for s, w in zip(top_strategies, weights)},
        'sharpe_ratio': float(ensemble_sharpe),
        'annual_return': float(ensemble_return),
        'annual_volatility': float(ensemble_vol),
        'constituent_strategies': [s[0] for s in top_strategies]
    }

def create_risk_parity_ensemble(top_strategies):
    """ãƒªã‚¹ã‚¯ãƒ‘ãƒªãƒ†ã‚£ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«"""
    volatilities = np.array([s[1]['annual_volatility'] for s in top_strategies])
    weights = (1/volatilities) / np.sum(1/volatilities)
    
    ensemble_return = np.sum(weights * [s[1]['annual_return'] for s in top_strategies])
    ensemble_vol = np.sqrt(np.sum((weights * volatilities)**2)) * 0.93
    ensemble_sharpe = ensemble_return / ensemble_vol if ensemble_vol > 0 else 0
    
    return {
        'weights': {s[0]: w for s, w in zip(top_strategies, weights)},
        'sharpe_ratio': float(ensemble_sharpe),
        'annual_return': float(ensemble_return),
        'annual_volatility': float(ensemble_vol),
        'constituent_strategies': [s[0] for s in top_strategies]
    }

@log_dataflow("çµæœåˆ†æãƒ»ä¿å­˜")
def analyze_and_save_results(strategy_results, market_data):
    """çµæœåˆ†æã¨ä¿å­˜"""
    
    log_step(
        "å®Ÿé¨“çµæœåˆ†æé–‹å§‹",
        inputs={'strategy_results': f"{len(strategy_results)}å¸‚å ´ã®çµæœ"},
        metadata={'analysis_types': ['cross_market_comparison', 'synthetic_vs_real', 'statistical_significance']}
    )
    
    # ã‚¯ãƒ­ã‚¹ãƒãƒ¼ã‚±ãƒƒãƒˆæ¯”è¼ƒ
    cross_market_analysis = perform_cross_market_analysis(strategy_results)
    
    # çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®š
    significance_tests = perform_significance_tests(strategy_results)
    
    # åˆæˆãƒ‡ãƒ¼ã‚¿ã¨ã®æ¯”è¼ƒï¼ˆå‰å›å®Ÿé¨“çµæœèª­ã¿è¾¼ã¿ï¼‰
    synthetic_comparison = compare_with_synthetic_results(strategy_results)
    
    # åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
    comprehensive_report = {
        'experiment_metadata': {
            'experiment_name': 'real_data_validation_v1',
            'execution_time': datetime.now().isoformat(),
            'markets_tested': list(strategy_results.keys()),
            'total_strategies': len(next(iter(strategy_results.values()))['individual_performance']),
            'data_period': '2y',
            'transaction_costs': 0.001
        },
        'market_data_summary': {
            market: {
                'symbols_count': len(data['symbols']),
                'trading_days': len(data['returns']),
                'avg_correlation': data['avg_correlation'],
                'quality_score': data['quality_metrics']['overall_score']
            }
            for market, data in market_data.items()
        },
        'strategy_results': strategy_results,
        'cross_market_analysis': cross_market_analysis,
        'significance_tests': significance_tests,
        'synthetic_comparison': synthetic_comparison,
        'key_findings': generate_key_findings(strategy_results, cross_market_analysis)
    }
    
    # çµæœä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = RESULTS_DIR / f"real_data_experiment_results_{timestamp}.json"
    
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(comprehensive_report, f, indent=2, ensure_ascii=False, default=str)
    
    log_file_operation(
        "å®Ÿé¨“çµæœä¿å­˜",
        str(results_file),
        data=comprehensive_report,
        success=True
    )
    
    return comprehensive_report, results_file

def perform_cross_market_analysis(strategy_results):
    """ã‚¯ãƒ­ã‚¹ãƒãƒ¼ã‚±ãƒƒãƒˆåˆ†æ"""
    
    analysis = {
        'best_strategies_by_market': {},
        'strategy_consistency': {},
        'market_characteristics': {}
    }
    
    # å¸‚å ´åˆ¥æœ€é«˜æˆ¦ç•¥
    for market, results in strategy_results.items():
        best_strategy = max(results['individual_performance'].keys(),
                          key=lambda k: results['individual_performance'][k]['sharpe_ratio'])
        analysis['best_strategies_by_market'][market] = {
            'strategy': best_strategy,
            'sharpe_ratio': results['individual_performance'][best_strategy]['sharpe_ratio'],
            'annual_return': results['individual_performance'][best_strategy]['annual_return']
        }
    
    # æˆ¦ç•¥ä¸€è²«æ€§ï¼ˆå…¨å¸‚å ´ã§ã®å¹³å‡æ€§èƒ½ï¼‰
    all_strategies = set()
    for results in strategy_results.values():
        all_strategies.update(results['individual_performance'].keys())
    
    for strategy in all_strategies:
        market_performances = []
        for market, results in strategy_results.items():
            if strategy in results['individual_performance']:
                market_performances.append(results['individual_performance'][strategy]['sharpe_ratio'])
        
        if market_performances:
            analysis['strategy_consistency'][strategy] = {
                'avg_sharpe': float(np.mean(market_performances)),
                'std_sharpe': float(np.std(market_performances)),
                'consistency_score': float(np.mean(market_performances) / (np.std(market_performances) + 0.01))
            }
    
    return analysis

def perform_significance_tests(strategy_results):
    """çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®š"""
    
    # å„æˆ¦ç•¥ã®ã‚·ãƒ£ãƒ¼ãƒ—æ¯”ä¿¡é ¼åŒºé–“è¨ˆç®—ï¼ˆãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ï¼‰
    significance_results = {}
    
    for market, results in strategy_results.items():
        market_significance = {}
        
        for strategy, performance in results['individual_performance'].items():
            # ç°¡æ˜“ä¿¡é ¼åŒºé–“è¨ˆç®—ï¼ˆæ­£è¦åˆ†å¸ƒè¿‘ä¼¼ï¼‰
            sharpe = performance['sharpe_ratio']
            trading_days = performance['trading_days']
            
            # ã‚·ãƒ£ãƒ¼ãƒ—æ¯”ã®æ¨™æº–èª¤å·®è¿‘ä¼¼
            se_sharpe = np.sqrt((1 + 0.5 * sharpe**2) / trading_days)
            
            # 95%ä¿¡é ¼åŒºé–“
            confidence_interval = [
                float(sharpe - 1.96 * se_sharpe),
                float(sharpe + 1.96 * se_sharpe)
            ]
            
            # çµ±è¨ˆçš„æœ‰æ„æ€§ï¼ˆã‚·ãƒ£ãƒ¼ãƒ—æ¯” > 0ï¼‰
            t_stat = sharpe / se_sharpe
            p_value = 2 * (1 - 0.5 * (1 + np.sign(t_stat) * np.sqrt(1 - np.exp(-2 * t_stat**2))))  # è¿‘ä¼¼
            
            market_significance[strategy] = {
                'sharpe_ratio': sharpe,
                'confidence_interval_95': confidence_interval,
                'standard_error': float(se_sharpe),
                't_statistic': float(t_stat),
                'p_value': float(max(0, min(1, p_value))),
                'is_significant': p_value < 0.05
            }
        
        significance_results[market] = market_significance
    
    return significance_results

def compare_with_synthetic_results(strategy_results):
    """åˆæˆãƒ‡ãƒ¼ã‚¿çµæœã¨ã®æ¯”è¼ƒ"""
    
    # åˆæˆãƒ‡ãƒ¼ã‚¿å®Ÿé¨“çµæœã‚’èª­ã¿è¾¼ã¿
    synthetic_files = list(PROJECT_ROOT.glob("*experiment_results*.json"))
    
    comparison = {
        'synthetic_results_found': len(synthetic_files) > 0,
        'performance_comparison': {},
        'insights': []
    }
    
    if synthetic_files:
        latest_synthetic = max(synthetic_files, key=lambda x: x.stat().st_mtime)
        
        try:
            with open(latest_synthetic, 'r', encoding='utf-8') as f:
                synthetic_data = json.load(f)
            
            # ä¸»è¦æˆ¦ç•¥ã®æ¯”è¼ƒ
            synthetic_strategies = synthetic_data.get('strategy_results', {}).get('individual_performances', {})
            
            for strategy in ['momentum_5d', 'mean_reversion_20d']:
                if strategy in synthetic_strategies:
                    synthetic_sharpe = synthetic_strategies[strategy]['sharpe_ratio']
                    
                    real_sharpes = []
                    for market_results in strategy_results.values():
                        if strategy in market_results['individual_performance']:
                            real_sharpes.append(market_results['individual_performance'][strategy]['sharpe_ratio'])
                    
                    if real_sharpes:
                        avg_real_sharpe = np.mean(real_sharpes)
                        comparison['performance_comparison'][strategy] = {
                            'synthetic_sharpe': synthetic_sharpe,
                            'real_sharpe_avg': float(avg_real_sharpe),
                            'performance_gap': float(avg_real_sharpe - synthetic_sharpe),
                            'real_vs_synthetic_ratio': float(avg_real_sharpe / synthetic_sharpe) if synthetic_sharpe != 0 else None
                        }
        
        except Exception as e:
            comparison['error'] = f"åˆæˆãƒ‡ãƒ¼ã‚¿æ¯”è¼ƒã‚¨ãƒ©ãƒ¼: {e}"
    
    return comparison

def generate_key_findings(strategy_results, cross_market_analysis):
    """ä¸»è¦ç™ºè¦‹äº‹é …ã®ç”Ÿæˆ"""
    
    findings = []
    
    # æœ€é«˜æ€§èƒ½æˆ¦ç•¥
    all_sharpes = []
    for market_results in strategy_results.values():
        for performance in market_results['individual_performance'].values():
            all_sharpes.append(performance['sharpe_ratio'])
    
    max_sharpe = max(all_sharpes)
    findings.append(f"æœ€é«˜ã‚·ãƒ£ãƒ¼ãƒ—æ¯”: {max_sharpe:.3f}")
    
    # å¸‚å ´é–“ä¸€è²«æ€§
    consistent_strategies = [
        s for s, metrics in cross_market_analysis['strategy_consistency'].items()
        if metrics['consistency_score'] > 1.0
    ]
    
    if consistent_strategies:
        findings.append(f"å…¨å¸‚å ´ã§ä¸€è²«ã—ã¦é«˜æ€§èƒ½: {', '.join(consistent_strategies[:3])}")
    
    # ãƒãƒ¼ã‚±ãƒƒãƒˆç‰¹æ€§
    best_markets = [
        market for market, info in cross_market_analysis['best_strategies_by_market'].items()
        if info['sharpe_ratio'] > 1.0
    ]
    
    if best_markets:
        findings.append(f"é«˜æ€§èƒ½å¸‚å ´: {', '.join(best_markets)}")
    
    return findings

@log_dataflow("å®Ÿãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å®Ÿé¨“çµ±åˆå®Ÿè¡Œ")
def run_real_data_validation_experiment():
    """å®Ÿãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å®Ÿé¨“ã®çµ±åˆå®Ÿè¡Œ"""
    
    experiment_start_time = time.time()
    
    log_step(
        "å®Ÿãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å®Ÿé¨“é–‹å§‹",
        metadata={
            'experiment_type': 'real_market_data_validation',
            'objectives': [
                'validate_synthetic_results',
                'cross_market_comparison', 
                'statistical_significance_testing',
                'realistic_performance_assessment'
            ]
        }
    )
    
    try:
        # ãƒ•ã‚§ãƒ¼ã‚º1: å®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†
        market_data = load_and_preprocess_real_data()
        
        if not market_data:
            raise ValueError("å®Ÿãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        # ãƒ•ã‚§ãƒ¼ã‚º2: æˆ¦ç•¥æ€§èƒ½ãƒ†ã‚¹ãƒˆ
        strategy_results = test_strategies_on_real_data(market_data)
        
        # ãƒ•ã‚§ãƒ¼ã‚º3: çµæœåˆ†æãƒ»ä¿å­˜
        comprehensive_report, results_file = analyze_and_save_results(strategy_results, market_data)
        
        experiment_duration = time.time() - experiment_start_time
        
        log_step(
            "å®Ÿãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å®Ÿé¨“å®Œäº†",
            outputs={'comprehensive_report': comprehensive_report},
            file_paths={'results_file': str(results_file)},
            metadata={
                'experiment_success': True,
                'duration_minutes': experiment_duration / 60,
                'markets_analyzed': len(market_data),
                'total_symbols': sum(len(data['symbols']) for data in market_data.values())
            }
        )
        
        return comprehensive_report, results_file
        
    except Exception as e:
        experiment_duration = time.time() - experiment_start_time
        
        log_step(
            "å®Ÿãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å®Ÿé¨“å¤±æ•—",
            metadata={
                'experiment_success': False,
                'error_type': type(e).__name__,
                'error_message': str(e),
                'duration_seconds': experiment_duration
            }
        )
        
        raise

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    print("ğŸš€ å®Ÿå¸‚å ´ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å®Ÿé¨“é–‹å§‹")
    print("=" * 80)
    
    try:
        comprehensive_report, results_file = run_real_data_validation_experiment()
        
        print("\n" + "=" * 80)
        print("ğŸ‰ å®Ÿãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å®Ÿé¨“å®Œäº†")
        print("=" * 80)
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print(f"ğŸ“Š åˆ†æå¸‚å ´æ•°: {len(comprehensive_report['market_data_summary'])}")
        print(f"ğŸ“ˆ ãƒ†ã‚¹ãƒˆæˆ¦ç•¥æ•°: {comprehensive_report['experiment_metadata']['total_strategies']}")
        
        # ä¸»è¦çµæœ
        if 'key_findings' in comprehensive_report:
            print("\nğŸ” ä¸»è¦ç™ºè¦‹äº‹é …:")
            for finding in comprehensive_report['key_findings']:
                print(f"  â€¢ {finding}")
        
        print(f"\nğŸ“ è©³ç´°çµæœ: {results_file}")
        print(f"ğŸ“‹ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: logs/dataflow_*.log")
        
        return True
        
    except Exception as e:
        print(f"\nğŸ’¥ å®Ÿé¨“å¤±æ•—: {e}")
        print(f"è©³ç´°: {traceback.format_exc()}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)