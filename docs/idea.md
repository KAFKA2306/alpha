
### ウキさんの発表: AIエージェントによる株価予測アーキテクチャ探索

伊藤さんに続き、本日は私の発表をさせていただきます。今回のセミナーテーマが「**投資戦略を量産せよ**」であるため、伊藤さんに触発され、私のチャンネルで行っていた内容を発表します。タイトルは「**株価予測の最強アーキテクチャはどれだ？ AIエージェントによる探索**」です。

**AIエージェントと投資戦略の定義**

**AIエージェント**と投資戦略についてです。**LLM**に投資戦略を考えさせることは、一昔前までは不可能でしたが、現在ではそれらしい考えを生成してくれます。例えば、**Chat GPT**に「株式投資で考えられる投資戦略を10個挙げてください」と質問すると、**バリュー投資**や**グロース投資**など、10個の戦略を返してきました。さらに踏み込み、**J-QuantsのAPI**を使って翌日の株価を予測し収益を得るための投資戦略を10個挙げるように依頼すると、APIのどのエンドポイントを使えばよいかまで含めて回答しました。データに基づいて考えさせる場合、これは**データサイエンス**の分野に非常に近くなります。結局、戦略とは、**特徴量の検討**や**モデリング**を行うことに帰着します。

私の発表における「**投資戦略**」の定義は、「**手持ちの情報を元に売買を判断できる根拠を算出するプロセス**」です。これは、情報から投資戦略を立て、売買の意思決定を行うという流れであり、人間、機械、**LLM**のいずれにも当てはまります。

人間が裁量で投資を行う場合、画面上のチャートを自身のドメイン知識でデータ処理し、過去の経験と照合して売買を決定します。機械が実行する場合、適切なデータを使用し、人間または**LLM**がドメイン知識を与え、モデルがパターンマッチングを行って意思決定します。今回は、このような定義における投資戦略の量産を試みました。

**戦略量産のリスクとアプローチ**

多数の戦略を作成し、その中で最もパフォーマンスの良いものだけを運用するのは避けるべきです。**金融データはばらつきが大きく**、ランキングトップのものは運による上振れの可能性が高く、その運の要素が後で必ず差し引かれるため危険です。安全な運用としては、**多数の戦略を作成し、分散運用する**のが妥当です。戦略間の相関に注意し、**均等ウェイト**にするか、**多数決モデル**にするのが安全です。

投資戦略の量産には、大きく分けて二つのアプローチがあります。
*   **帰納法（データドリブン）**：具体的な事例や観察から一般的な法則や結論を導き出す方法です。データの意味を深く考えず、多くのパターンを試行し、後から理由を考察します。多数のデータを扱うため、偶然良い結果が得られるという**バイアス**に注意が必要です。
*   **演繹法（セオリードリブン）**：一般的なルールや原則から結論を導き出し、それを検証する方法です。この方法は思考の枠組みに囚われやすく、戦略の量産には不向きだと考えられます。

今回は、できるだけ多くの戦略を生成するため、**帰納法（データドリブン）**のアプローチを選択しました。

**投資戦略の量産事例**

投資戦略を量産する事例を3つご紹介します。
1.  **トレーダーカンパニー法（伊藤さんの研究）**：2項演算子と活性化関数、多項式を組み合わせて多数の予測器を作成します。これは**帰納的なアプローチ**に近く、例えば最大値、最小値、不等号などの演算子と、ReLUだけでなくサイン関数などの活性化関数を組み合わせます。
2.  **GANs（敵対的生成ネットワーク）**：GANを用いて時系列サンプルを生成し、それを使って投資戦略のパラメーターをチューニングします。生成されたサンプルに応じて様々なパラメーターパターンが作成され、多数の戦略としてアンサンブルされます。これは**パラメーターチューニング**の事例です。
3.  **Automated Strategy Finding with LLM**：**LLM**が論文や金融記事からアルファに関する数式（ファクターズのような）を抽出し、**マルチモーダルデータ**と組み合わせて、最終的に**シャープレシオ**で戦略を選別します。これは**演繹的なアプローチ**と言えるでしょう。

これらの事例を踏まえると、投資戦略の量産には一般的なパターンが3つあります。
1.  **ランダムな組み合わせ**
2.  **パラメーターの探索**
3.  **LLM**や**エージェント**の活用

**量産方法のメリット・デメリットと今回の方針**

それぞれの量産方法にはメリット・デメリットがあります。
*   **ランダムな組み合わせ**：手持ちのデータやコンポーネント（ブロック）をランダムに選択し、多数のパターンを作成します。**ランダムフォレスト**もこの一種です。デメリットは、組み合わせの整合性を保つために機械的な設定になりがちな点です。
*   **パラメーター探索**：ウィンドウ設定を変えるだけで簡単に戦略を増やすことができますが、探索範囲が狭く、戦略の多様性に欠けがちです。
*   **LLM/AIエージェント**：プロンプトを変更することで、従来の枠組みを超えた柔軟なアイデアを生成できます。しかし、ある程度生成すると似たような提案が増え、多様性が失われる傾向があります。コンテキストレスで多様性を確保しようとしても、結局ランダムな組み合わせが必要になるなどのデメリットがあります。

私の今回の方針は、**AIエージェントとランダムな組み合わせを併用**して投資戦略を量産することです。AIエージェント単独では多様性が不足するため、人間がフレームワークを指定してそれを補います。フレームワークの組み込みは、データのランダムネス、ドメイン知識（プロンプト）の変化、モデリングなどで行われます。今回は、データとパターンマッチングの部分を全て統合し、**エンドツーエンドのランダムモデルを量産**することに焦点を当てます。さらに、**LLM**を使ってドメイン知識をコードに変換し、エージェントが量産の自動化を支援します。

提案するアプローチは「**アルファアーキテクチャエージェント**」と名付けた、**ニューラルネットワーク**を組み合わせる試みです。事前に**時系列解析**や**金融ドメインのブロック**を作成しておきます。これらのブロックは主に**LLM**が生成し、人間がチェックして成形しました。**AIエージェント**は、これらのドメインブロックを整合性が取れるように組み合わせて、アーキテクチャを量産します。

**ドメインブロックの詳細とフレームワーク**

ドメインブロックのコード生成には基本的に**Chat GPT**を使用し、私自身がチェックと成形を行いました。大きく分けて3つのパターンで生成しています。
1.  時系列ニューラルネットの論文からアーキテクチャをブロックに分割。
2.  **LLM**自身に金融に効果的なアーキテクチャを考案させる。
3.  私のドメイン知識を与え、それに合ったアーキテクチャを作成させる。

約50個の**ニューラルネットワーク**に組み込むブロックを作成しました。
*   **企画化系ブロック**：**Batch Norm**、**Layer Norm**、**Adaptive Instance Norm**（パラメーターとして学習させる標準化）、**Demean**（平均を引く）。
*   **成分抽出ブロック**：**PCA**（上位主成分でデータ再構築）、**Fourier Feature Extract**（フーリエ変換で上位N周波数成分から入力再構築、**Performer**で使用）。
*   **ミキシングブロック**：時間方向とチャネル方向のミキシング。
*   **入力エンコーディングブロック**：1層、3層、畳み込みを使った3層。
*   **金融ドメインブロック**：
    *   **Multi-Time Frame Block**：複数のカーネルサイズで畳み込みを行い、マルチタイムフレームの特徴を抽出。
    *   **Lead-Lag Block**：時間軸をずらして畳み込みを行い、リード・ラグ構造を抽出。
    *   **Regime Detection Block**。
    *   **PCA Block**：主成分のファクターリターンを抽出するレジームブロック。
*   **特徴方向統合ブロック**：**Average Pooling**、最新値取得、最新値移動平均からの乖離、**Linear**、**Convolution**。
*   **時間方向統合ブロック**。
*   **銘柄ごとの特徴付けブロック**：チャネル情報に基づく**クラスタリング**、**Transformer**を用いた銘柄方向の相関関係抽出（アテンション）、**PCA Exposure**（ファクターエクスポージャー算出）。
*   **Transformer用アテンションブロック**：標準的な**Multi-Head Attention**、**Informer**に採用されている**ProbSparse Self-Attention**、**Autoformer**の**Auto-Correlation**、**Crossformer**の**Cross Self-Attention**。
*   **Transformerのフィードフォワード層**：標準的なものと**Gated**。
*   **時間埋め込み系ブロック**：**Positional Encoding**、**Time Embedding**。
*   **特徴ブロック**：**Transformer**、**LSTM**、**GRU**（リカレントユニット）。**Mamba**は収束しなかったため今回の検討からは外れました。
*   **予測ヘッドブロック**：**MSE損失**を用いる単一出力ブロック、分類問題のための3つ出力ブロック。

**AIエージェント**はこれらのブロックを組み合わせてアーキテクチャを生成します。プロンプトは、**LLM**が生成したブロックコードを文字列として与え、**AIエージェント**がシェイプの整合性を考慮してブロックを選定し、全体のコードを作成するよう指示します。

今回のフレームワークは、**日本株のデイリーロングショート**を想定しています。終値から終値までの時系列予測を行います。入力はバッチ、銘柄数、シーケンス長252のリターン系列です。**ニューラルネットワーク**でその日の予測を行い、上位5%をロング、下位5%をショートすることで**ロングショートポートフォリオ**を構築します。

**実験結果と考察**

学習期間は**2017年から2023年**、検証期間は**2024年から2025年2月末**と設定し、この検証期間のパフォーマンスで戦略をランキングしました。70個の戦略を訓練・バックテストし、**シャープレシオ**を算出しました。最高のシャープレシオは**1.3程度**で、その後は1.2、1.0を切るものが多く、全体的に高いパフォーマンスは多くありませんでした。しかし、分析すると興味深い傾向が見られました。

効果のあるブロックのランキングを作成しました。これは、特定のブロックが含まれる戦略群と含まれない戦略群の**シャープレシオの平均の差分**に基づいており、ブロックが加わるとシャープレシオが改善される傾向を示しています。これはあくまで参考データで、試行回数の不足やシャープレシオの差分比較の限界があることに留意が必要です。

良かったブロックと悪かったブロックをピックアップした結果は以下の通りです。
*   **良かったブロック**：
    *   1位は「**特徴方向を統合するConvolution 3層ブロック**」で、含まれる戦略と含まれない戦略の平均シャープレシオの差が**0.5**でした。3位にも同様の1層ブロックがあり、特徴方向の情報を統合するには**畳み込みネットワーク**が有効な傾向が見られました。
    *   **PCA Exposure**ブロックも有効でした。リターン系列しか使っていない中で、ファクター的な情報を抽出するのに役立ったと考えられます。
*   **悪かったブロック**：
    *   「**シーケンス統合ブロック**」（3層Linear）はパフォーマンスが良くありませんでした。筆者の知見と一致し、シーケンスは「曖昧に統合」する方が良いという考え（例：**Average Pooling**）が示唆されました。
    *   **Transformer**はあまり良くありませんでした。
    *   入力チャネルを統合する**Encoderブロック**も良くありませんでした。
    *   **PCA**（上位主成分のみ抽出）も悪影響を与え、おそらく分散が大きくなったためと考えられます。

直近（2024年3月〜6月末）の**フォワード期間**の成績では、**上位20戦略を均等ウェイトでアンサンブル**した結果、**シャープレシオ約2.2**を達成しました。ランキング時よりも好調なものが多く、上位20戦略のうちマイナスになったのは5つだけで、中にはシャープレシオ4.8〜4.9というものもありました。2024年4月の関税ショックで一時的に落ち込みましたが、その後盛り返し安定しています。

**まとめ**

この企画は、正直言って失敗すると思っていましたが、予想以上に良い知見とパフォーマンスが得られました。**ニューラルネットワーク**は非常に強力です。**LightGBM**などでランキングを作成していたら、もっと乱雑になったかもしれません。検証期間で上位に来たモデルは、**アウトオブサンプル**でも予測性能が維持されており、何らかの普遍的な特徴を捉えている可能性があります。詳細な裏話は後日私のチャンネルで配信します。

北山さんから「50個のブロックを作るのは大変」というコメントがありましたが、実際は私が論文のURLを与えて**LLM**にブロックを作成させたため、大したことはしていません。**ニューラルアーキテクチャサーチ（NAS）**との比較もありましたが、これだけのブロックが作れている時点で**NAS**はすごいと感じました。

### 質疑応答と今後の議論

北山さんから伊藤さんへの質問です。**Claude Code**のような**LLM**が戦略を量産する上で有利な状況を設計するためには、人間よりも多くの情報（例えば**MCPサーバー**を通じて外部情報）を与えるべきではないかという点についてです。**Claude Code**自身も、内部で**grep**ツールを駆使して大量のソースコードを扱っている点が興味深いとのことです。伊藤さんは、「**スキルライブラリー**」のように、自身が作成したアーキテクチャを参照していくのが一番良いと述べました。

**MCPサーバー**に関しては、ウキさんはまだ手を出していませんが、構造化された形で欲しいデータを一気に返してくれる**MCPサーバー**を求めています。特に、**オルタナティブデータ**や**マクロデータ**（雇用統計、PMIなど）が重要で、現状は自分でスクレイピングしている状況です。伊藤さんは、**マクロボンドさん**のようなマクロデータを整理して提供するサービスは、株価データよりもポテンシャルを秘めた戦略につながる可能性があると述べました。**グローバルマクロデータベース**のように統一されたデータも活用できます。ただし、これらのデータは速報性に欠けるため、実際の運用では難しい面もありますが、研究やアイデア探索には非常に有用です。