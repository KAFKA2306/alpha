# 実験フレームワーク

## 概要
Alpha Architecture Agentにおける実験フレームワークについて詳細に説明いたします。本フレームワークは、AIエージェントによるアーキテクチャ生成からバックテストまでの一連の実験を自動化し、結果を体系的に管理することを目的としております。

## 実験フレームワークの構成

### アーキテクチャ概要
```
実験フレームワーク
├── 実験設計モジュール
│   ├── 実験計画策定
│   ├── パラメータ設定
│   └── 実験条件定義
├── 実験実行モジュール
│   ├── アーキテクチャ生成
│   ├── モデル訓練
│   ├── バックテスト実行
│   └── 結果収集
├── 実験管理モジュール
│   ├── 実験状況監視
│   ├── リソース管理
│   └── エラーハンドリング
└── 結果分析モジュール
    ├── 性能評価
    ├── 統計分析
    └── 可視化
```

## 実験設計

### 1. 実験設定クラス

#### 基本設定
```python
from dataclasses import dataclass
from typing import Dict, List, Optional, Any
import yaml

@dataclass
class ExperimentConfig:
    """実験設定のデータクラス"""
    
    # 実験基本情報
    name: str
    description: str
    version: str
    
    # アーキテクチャ生成設定
    architecture_config: Dict[str, Any]
    
    # 訓練設定
    training_config: Dict[str, Any]
    
    # バックテスト設定
    backtest_config: Dict[str, Any]
    
    # データ設定
    data_config: Dict[str, Any]
    
    # 実験実行設定
    execution_config: Dict[str, Any]
    
    @classmethod
    def from_yaml(cls, config_path: str) -> 'ExperimentConfig':
        """YAMLファイルから設定を読み込み"""
        with open(config_path, 'r', encoding='utf-8') as f:
            config_data = yaml.safe_load(f)
        return cls(**config_data)
    
    def to_yaml(self, output_path: str) -> None:
        """設定をYAMLファイルに保存"""
        with open(output_path, 'w', encoding='utf-8') as f:
            yaml.dump(self.__dict__, f, default_flow_style=False)
```

#### 設定例
```yaml
# experiment_config.yaml
name: "alpha_architecture_experiment_v1"
description: "AIエージェントによるアーキテクチャ探索実験"
version: "1.0.0"

architecture_config:
  generation_method: "ai_agent"  # ai_agent, random, hybrid
  num_architectures: 100
  complexity_range: [5, 20]
  diversity_threshold: 0.8
  generation_timeout: 3600

training_config:
  epochs: 100
  batch_size: 32
  learning_rate: 0.001
  validation_split: 0.2
  early_stopping: true
  patience: 10

backtest_config:
  start_date: "2020-01-01"
  end_date: "2024-06-30"
  validation_start: "2024-01-01"
  validation_end: "2024-06-30"
  initial_capital: 10000000
  long_percentage: 0.05
  short_percentage: 0.05
  rebalance_frequency: "daily"

data_config:
  symbols: ["7203.T", "9984.T", "6758.T"]
  features: ["ohlcv", "technical_indicators"]
  sequence_length: 252
  prediction_horizon: 1

execution_config:
  max_parallel_jobs: 4
  resource_limits:
    max_memory: "8GB"
    max_gpu_memory: "4GB"
  retry_count: 3
  timeout: 7200
```

### 2. 実験計画策定

#### 実験計画クラス
```python
class ExperimentPlanner:
    """実験計画策定クラス"""
    
    def __init__(self, config: ExperimentConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
    
    def create_experiment_plan(self) -> Dict:
        """実験計画の作成"""
        plan = {
            'experiment_id': self._generate_experiment_id(),
            'phases': self._define_phases(),
            'resource_requirements': self._estimate_resources(),
            'timeline': self._estimate_timeline(),
            'success_criteria': self._define_success_criteria()
        }
        
        return plan
    
    def _define_phases(self) -> List[Dict]:
        """実験フェーズの定義"""
        phases = [
            {
                'name': 'data_preparation',
                'description': 'データ準備・前処理',
                'duration': 300,  # 秒
                'dependencies': []
            },
            {
                'name': 'architecture_generation',
                'description': 'アーキテクチャ生成',
                'duration': 3600,
                'dependencies': ['data_preparation']
            },
            {
                'name': 'model_training',
                'description': 'モデル訓練',
                'duration': 7200,
                'dependencies': ['architecture_generation']
            },
            {
                'name': 'backtesting',
                'description': 'バックテスト実行',
                'duration': 1800,
                'dependencies': ['model_training']
            },
            {
                'name': 'analysis',
                'description': '結果分析',
                'duration': 600,
                'dependencies': ['backtesting']
            }
        ]
        
        return phases
    
    def _estimate_resources(self) -> Dict:
        """リソース要件の推定"""
        num_architectures = self.config.architecture_config['num_architectures']
        
        return {
            'cpu_cores': min(8, num_architectures // 10),
            'memory_gb': min(16, num_architectures // 5),
            'gpu_memory_gb': 4,
            'disk_gb': 10
        }
    
    def _estimate_timeline(self) -> Dict:
        """実行時間の推定"""
        phases = self._define_phases()
        total_duration = sum(phase['duration'] for phase in phases)
        
        return {
            'estimated_duration': total_duration,
            'phases': {phase['name']: phase['duration'] for phase in phases}
        }
    
    def _define_success_criteria(self) -> Dict:
        """成功基準の定義"""
        return {
            'minimum_architectures': 50,
            'minimum_sharpe_ratio': 0.5,
            'maximum_correlation': 0.9,
            'completion_rate': 0.8
        }
```

### 3. パラメータ最適化

#### グリッドサーチ
```python
class ParameterOptimizer:
    """パラメータ最適化クラス"""
    
    def __init__(self, config: ExperimentConfig):
        self.config = config
    
    def grid_search(self, param_grid: Dict) -> List[Dict]:
        """グリッドサーチによるパラメータ探索"""
        from itertools import product
        
        param_combinations = []
        keys = list(param_grid.keys())
        values = list(param_grid.values())
        
        for combination in product(*values):
            param_dict = dict(zip(keys, combination))
            param_combinations.append(param_dict)
        
        return param_combinations
    
    def bayesian_optimization(self, param_space: Dict, n_trials: int = 100) -> List[Dict]:
        """ベイズ最適化によるパラメータ探索"""
        import optuna
        
        def objective(trial):
            # パラメータの提案
            params = {}
            for param_name, param_config in param_space.items():
                if param_config['type'] == 'float':
                    params[param_name] = trial.suggest_float(
                        param_name, 
                        param_config['low'], 
                        param_config['high']
                    )
                elif param_config['type'] == 'int':
                    params[param_name] = trial.suggest_int(
                        param_name, 
                        param_config['low'], 
                        param_config['high']
                    )
                elif param_config['type'] == 'categorical':
                    params[param_name] = trial.suggest_categorical(
                        param_name, 
                        param_config['choices']
                    )
            
            # 実験実行
            result = self._run_experiment_with_params(params)
            
            # 目的関数値を返す（シャープレシオを最大化）
            return result['sharpe_ratio']
        
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=n_trials)
        
        return study.trials
```

## 実験実行

### 1. 実験実行エンジン

#### メインエンジン
```python
class ExperimentRunner:
    """実験実行エンジン"""
    
    def __init__(self, config: ExperimentConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.mlflow_client = MLflowClient()
        self.results = {}
    
    async def run_experiment(self) -> Dict:
        """実験の実行"""
        experiment_id = self._create_mlflow_experiment()
        
        try:
            # 実験開始
            self.logger.info(f"実験開始: {self.config.name}")
            start_time = time.time()
            
            # フェーズ1: データ準備
            data = await self._prepare_data()
            
            # フェーズ2: アーキテクチャ生成
            architectures = await self._generate_architectures()
            
            # フェーズ3: モデル訓練
            models = await self._train_models(architectures, data)
            
            # フェーズ4: バックテスト
            backtest_results = await self._run_backtests(models, data)
            
            # フェーズ5: 結果分析
            analysis_results = await self._analyze_results(backtest_results)
            
            # 実験完了
            end_time = time.time()
            duration = end_time - start_time
            
            self.logger.info(f"実験完了: {duration:.2f}秒")
            
            return {
                'experiment_id': experiment_id,
                'status': 'completed',
                'duration': duration,
                'architectures': architectures,
                'models': models,
                'backtest_results': backtest_results,
                'analysis_results': analysis_results
            }
            
        except Exception as e:
            self.logger.error(f"実験失敗: {e}")
            return {
                'experiment_id': experiment_id,
                'status': 'failed',
                'error': str(e)
            }
    
    async def _prepare_data(self) -> Dict:
        """データ準備"""
        self.logger.info("データ準備開始")
        
        # データ収集
        collector = DataCollector(self.config.data_config)
        raw_data = await collector.collect_data()
        
        # 特徴量生成
        feature_engineer = FeatureEngineer()
        features = feature_engineer.create_features(raw_data)
        
        # 訓練・テスト分割
        train_data, test_data = self._split_data(features)
        
        return {
            'raw_data': raw_data,
            'features': features,
            'train_data': train_data,
            'test_data': test_data
        }
    
    async def _generate_architectures(self) -> List[Dict]:
        """アーキテクチャ生成"""
        self.logger.info("アーキテクチャ生成開始")
        
        generator = ArchitectureGenerator(self.config.architecture_config)
        architectures = await generator.generate_architectures()
        
        self.logger.info(f"生成されたアーキテクチャ数: {len(architectures)}")
        
        return architectures
    
    async def _train_models(self, architectures: List[Dict], data: Dict) -> List[Dict]:
        """モデル訓練"""
        self.logger.info("モデル訓練開始")
        
        trainer = ModelTrainer(self.config.training_config)
        models = []
        
        for i, architecture in enumerate(architectures):
            try:
                model = await trainer.train_model(architecture, data)
                models.append(model)
                self.logger.info(f"モデル {i+1}/{len(architectures)} 訓練完了")
            except Exception as e:
                self.logger.error(f"モデル {i+1} 訓練失敗: {e}")
        
        return models
    
    async def _run_backtests(self, models: List[Dict], data: Dict) -> List[Dict]:
        """バックテスト実行"""
        self.logger.info("バックテスト開始")
        
        backtester = Backtester(self.config.backtest_config)
        results = []
        
        for i, model in enumerate(models):
            try:
                result = await backtester.run_backtest(model, data)
                results.append(result)
                self.logger.info(f"バックテスト {i+1}/{len(models)} 完了")
            except Exception as e:
                self.logger.error(f"バックテスト {i+1} 失敗: {e}")
        
        return results
    
    async def _analyze_results(self, results: List[Dict]) -> Dict:
        """結果分析"""
        self.logger.info("結果分析開始")
        
        analyzer = ResultAnalyzer()
        analysis = analyzer.analyze_results(results)
        
        return analysis
```

### 2. 並列実行管理

#### 並列実行クラス
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed

class ParallelExperimentRunner:
    """並列実験実行クラス"""
    
    def __init__(self, config: ExperimentConfig):
        self.config = config
        self.max_workers = config.execution_config.get('max_parallel_jobs', 4)
        self.semaphore = asyncio.Semaphore(self.max_workers)
    
    async def run_parallel_experiments(self, experiment_configs: List[ExperimentConfig]) -> List[Dict]:
        """並列実験実行"""
        tasks = []
        
        for config in experiment_configs:
            task = self._run_single_experiment(config)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return results
    
    async def _run_single_experiment(self, config: ExperimentConfig) -> Dict:
        """単一実験の実行（セマフォ制御）"""
        async with self.semaphore:
            runner = ExperimentRunner(config)
            return await runner.run_experiment()
    
    def run_parallel_training(self, architectures: List[Dict], data: Dict) -> List[Dict]:
        """並列モデル訓練"""
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = []
            
            for architecture in architectures:
                future = executor.submit(self._train_single_model, architecture, data)
                futures.append(future)
            
            results = []
            for future in as_completed(futures):
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    self.logger.error(f"並列訓練エラー: {e}")
                    results.append(None)
            
            return results
    
    def _train_single_model(self, architecture: Dict, data: Dict) -> Dict:
        """単一モデルの訓練"""
        trainer = ModelTrainer(self.config.training_config)
        return trainer.train_model(architecture, data)
```

## 実験管理

### 1. 実験追跡

#### MLflow統合
```python
import mlflow
import mlflow.pytorch

class ExperimentTracker:
    """実験追跡クラス"""
    
    def __init__(self, tracking_uri: str):
        mlflow.set_tracking_uri(tracking_uri)
        self.client = mlflow.MlflowClient()
    
    def start_experiment(self, experiment_name: str) -> str:
        """実験開始"""
        experiment_id = mlflow.create_experiment(experiment_name)
        return experiment_id
    
    def log_experiment_run(self, experiment_id: str, run_data: Dict) -> str:
        """実験ランの記録"""
        with mlflow.start_run(experiment_id=experiment_id):
            # パラメータの記録
            for key, value in run_data.get('parameters', {}).items():
                mlflow.log_param(key, value)
            
            # メトリクスの記録
            for key, value in run_data.get('metrics', {}).items():
                mlflow.log_metric(key, value)
            
            # アーティファクトの記録
            for artifact_path, artifact_data in run_data.get('artifacts', {}).items():
                mlflow.log_artifact(artifact_path)
            
            # モデルの記録
            if 'model' in run_data:
                mlflow.pytorch.log_model(run_data['model'], "model")
            
            return mlflow.active_run().info.run_id
    
    def get_experiment_results(self, experiment_id: str) -> List[Dict]:
        """実験結果の取得"""
        runs = self.client.search_runs(experiment_ids=[experiment_id])
        
        results = []
        for run in runs:
            result = {
                'run_id': run.info.run_id,
                'status': run.info.status,
                'parameters': run.data.params,
                'metrics': run.data.metrics,
                'start_time': run.info.start_time,
                'end_time': run.info.end_time
            }
            results.append(result)
        
        return results
```

### 2. リソース管理

#### リソース監視
```python
import psutil
import GPUtil

class ResourceMonitor:
    """リソース監視クラス"""
    
    def __init__(self, limits: Dict):
        self.limits = limits
        self.alerts = []
    
    def check_resources(self) -> Dict:
        """リソース使用量の確認"""
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        # GPU使用量
        gpu_usage = []
        try:
            gpus = GPUtil.getGPUs()
            for gpu in gpus:
                gpu_usage.append({
                    'id': gpu.id,
                    'usage': gpu.load * 100,
                    'memory_usage': gpu.memoryUtil * 100
                })
        except:
            gpu_usage = []
        
        status = {
            'cpu_percent': cpu_percent,
            'memory_percent': memory.percent,
            'memory_used_gb': memory.used / (1024**3),
            'disk_percent': disk.percent,
            'gpu_usage': gpu_usage
        }
        
        # アラートチェック
        self._check_alerts(status)
        
        return status
    
    def _check_alerts(self, status: Dict) -> None:
        """アラート条件のチェック"""
        if status['cpu_percent'] > 90:
            self.alerts.append("CPU使用率が90%を超えました")
        
        if status['memory_percent'] > 90:
            self.alerts.append("メモリ使用率が90%を超えました")
        
        if status['disk_percent'] > 80:
            self.alerts.append("ディスク使用率が80%を超えました")
    
    def wait_for_resources(self, required_memory_gb: float) -> None:
        """リソース確保まで待機"""
        while True:
            status = self.check_resources()
            available_memory = (100 - status['memory_percent']) / 100 * psutil.virtual_memory().total / (1024**3)
            
            if available_memory >= required_memory_gb:
                break
            
            time.sleep(10)  # 10秒待機
```

## 結果分析

### 1. 性能評価

#### 評価指標計算
```python
class PerformanceAnalyzer:
    """性能評価クラス"""
    
    def __init__(self):
        self.metrics = {}
    
    def calculate_metrics(self, returns: pd.Series) -> Dict:
        """各種評価指標の計算"""
        metrics = {}
        
        # 基本統計
        metrics['total_return'] = (1 + returns).prod() - 1
        metrics['annualized_return'] = (1 + returns.mean()) ** 252 - 1
        metrics['volatility'] = returns.std() * np.sqrt(252)
        
        # リスク調整後指標
        metrics['sharpe_ratio'] = metrics['annualized_return'] / metrics['volatility']
        metrics['sortino_ratio'] = metrics['annualized_return'] / (returns[returns < 0].std() * np.sqrt(252))
        
        # ドローダウン指標
        cumulative_returns = (1 + returns).cumprod()
        running_max = cumulative_returns.cummax()
        drawdown = (cumulative_returns - running_max) / running_max
        
        metrics['max_drawdown'] = drawdown.min()
        metrics['calmar_ratio'] = metrics['annualized_return'] / abs(metrics['max_drawdown'])
        
        # その他の指標
        metrics['hit_rate'] = (returns > 0).mean()
        metrics['average_win'] = returns[returns > 0].mean()
        metrics['average_loss'] = returns[returns < 0].mean()
        metrics['profit_factor'] = abs(returns[returns > 0].sum() / returns[returns < 0].sum())
        
        return metrics
    
    def compare_strategies(self, results: List[Dict]) -> pd.DataFrame:
        """戦略比較"""
        comparison_data = []
        
        for i, result in enumerate(results):
            metrics = self.calculate_metrics(result['returns'])
            metrics['strategy_id'] = i
            metrics['architecture_name'] = result.get('architecture_name', f'Strategy_{i}')
            comparison_data.append(metrics)
        
        return pd.DataFrame(comparison_data)
```

### 2. 統計分析

#### 統計検定
```python
from scipy import stats

class StatisticalAnalyzer:
    """統計分析クラス"""
    
    def __init__(self):
        pass
    
    def performance_significance_test(self, returns1: pd.Series, returns2: pd.Series) -> Dict:
        """パフォーマンスの有意性検定"""
        # t検定
        t_stat, t_p_value = stats.ttest_ind(returns1, returns2)
        
        # Wilcoxon順位和検定
        u_stat, u_p_value = stats.mannwhitneyu(returns1, returns2)
        
        # Kolmogorov-Smirnov検定
        ks_stat, ks_p_value = stats.ks_2samp(returns1, returns2)
        
        return {
            't_test': {'statistic': t_stat, 'p_value': t_p_value},
            'mann_whitney': {'statistic': u_stat, 'p_value': u_p_value},
            'ks_test': {'statistic': ks_stat, 'p_value': ks_p_value}
        }
    
    def correlation_analysis(self, returns_matrix: pd.DataFrame) -> Dict:
        """相関分析"""
        correlation_matrix = returns_matrix.corr()
        
        # 平均相関
        avg_correlation = correlation_matrix.values[np.triu_indices_from(correlation_matrix.values, k=1)].mean()
        
        # 最大相関
        max_correlation = correlation_matrix.values[np.triu_indices_from(correlation_matrix.values, k=1)].max()
        
        return {
            'correlation_matrix': correlation_matrix,
            'average_correlation': avg_correlation,
            'max_correlation': max_correlation
        }
```

## 使用例

### 基本的な実験実行
```python
# 実験設定の読み込み
config = ExperimentConfig.from_yaml('config/experiment_config.yaml')

# 実験実行
runner = ExperimentRunner(config)
results = await runner.run_experiment()

# 結果確認
print(f"実験状況: {results['status']}")
print(f"実行時間: {results['duration']:.2f}秒")
print(f"生成アーキテクチャ数: {len(results['architectures'])}")
```

### 並列実験実行
```python
# 複数の実験設定
configs = [
    ExperimentConfig.from_yaml('config/experiment_1.yaml'),
    ExperimentConfig.from_yaml('config/experiment_2.yaml'),
    ExperimentConfig.from_yaml('config/experiment_3.yaml')
]

# 並列実行
parallel_runner = ParallelExperimentRunner(configs[0])
results = await parallel_runner.run_parallel_experiments(configs)

# 結果比較
analyzer = PerformanceAnalyzer()
comparison = analyzer.compare_strategies([r['backtest_results'] for r in results])
print(comparison.sort_values('sharpe_ratio', ascending=False))
```

### パラメータ最適化
```python
# パラメータ空間の定義
param_space = {
    'learning_rate': {'type': 'float', 'low': 0.0001, 'high': 0.01},
    'batch_size': {'type': 'int', 'low': 16, 'high': 128},
    'num_layers': {'type': 'int', 'low': 1, 'high': 5},
    'hidden_dim': {'type': 'categorical', 'choices': [32, 64, 128, 256]}
}

# 最適化実行
optimizer = ParameterOptimizer(config)
best_params = optimizer.bayesian_optimization(param_space, n_trials=100)

print(f"最適パラメータ: {best_params}")
```

このフレームワークにより、体系的で再現性のある実験を効率的に実行することができます。