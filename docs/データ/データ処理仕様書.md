# データ処理仕様書

## 概要
Alpha Architecture Agentにおけるデータ処理パイプラインの詳細仕様について説明いたします。日本株式市場のデータを中心に、効率的で信頼性の高いデータ処理システムを構築しております。

## データソース

### 1. J-Quants API
**用途**: 日本株式の基本データ取得
```python
# 設定例
j_quants_config = {
    "base_url": "https://api.j-quants.com/v1",
    "api_key": os.environ.get("JQUANTS_API_KEY"),
    "rate_limit": 1000,  # 1時間あたり
    "retry_count": 3
}
```

**取得可能データ**:
- 株価データ（OHLCV）
- 財務データ
- 企業情報
- 配当情報
- 株式分割情報

### 2. Yahoo Finance API
**用途**: 補助データおよび海外市場データ
```python
# 設定例
yahoo_config = {
    "symbols": ["^N225", "^GSPC", "^DJI"],
    "interval": "1d",
    "period": "2y"
}
```

**取得可能データ**:
- 指数データ
- 為替データ
- 商品データ
- 海外株式データ

### 3. 自社データ
**用途**: 独自指標および加工データ
- 技術指標
- センチメント指標
- マクロ経済指標

## データ収集システム

### 収集フロー
```python
class DataCollector:
    """データ収集の中核クラス"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.storage = DataStorage(config['storage'])
    
    async def collect_stock_data(self, symbols: List[str], 
                               start_date: str, 
                               end_date: str) -> Dict:
        """株式データの収集"""
        results = {}
        
        for symbol in symbols:
            try:
                # J-Quants APIから基本データ取得
                ohlcv_data = await self._fetch_ohlcv(symbol, start_date, end_date)
                
                # 技術指標の計算
                technical_data = self._calculate_technical_indicators(ohlcv_data)
                
                # データ統合
                combined_data = pd.concat([ohlcv_data, technical_data], axis=1)
                
                # 品質チェック
                validated_data = self._validate_data(combined_data)
                
                # 保存
                await self.storage.save_data(symbol, validated_data)
                
                results[symbol] = {
                    'status': 'success',
                    'records': len(validated_data),
                    'date_range': [start_date, end_date]
                }
                
            except Exception as e:
                self.logger.error(f"データ収集失敗 {symbol}: {e}")
                results[symbol] = {
                    'status': 'error',
                    'error': str(e)
                }
        
        return results
```

### 収集スケジュール
```python
# 定期収集設定
collection_schedule = {
    "daily_update": {
        "time": "18:00",  # 市場終了後
        "data_types": ["ohlcv", "volume", "technical_indicators"],
        "symbols": "all_active"
    },
    "weekly_update": {
        "day": "sunday",
        "time": "02:00",
        "data_types": ["fundamental", "corporate_actions"],
        "symbols": "all_listed"
    },
    "monthly_update": {
        "day": 1,
        "time": "01:00",
        "data_types": ["sector_data", "macro_indicators"],
        "symbols": "indices"
    }
}
```

## データ変換・加工

### 1. 基本的な前処理

#### 欠損値処理
```python
def handle_missing_values(df: pd.DataFrame, method: str = 'forward_fill') -> pd.DataFrame:
    """欠損値の処理"""
    if method == 'forward_fill':
        return df.fillna(method='ffill')
    elif method == 'interpolate':
        return df.interpolate(method='linear')
    elif method == 'drop':
        return df.dropna()
    else:
        raise ValueError(f"未対応の処理方法: {method}")
```

#### 外れ値除去
```python
def remove_outliers(df: pd.DataFrame, method: str = 'zscore', threshold: float = 3.0) -> pd.DataFrame:
    """外れ値の除去"""
    if method == 'zscore':
        z_scores = np.abs(stats.zscore(df.select_dtypes(include=[np.number])))
        return df[(z_scores < threshold).all(axis=1)]
    elif method == 'iqr':
        Q1 = df.quantile(0.25)
        Q3 = df.quantile(0.75)
        IQR = Q3 - Q1
        return df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]
```

### 2. 特徴量エンジニアリング

#### 技術指標の計算
```python
class TechnicalIndicators:
    """技術指標計算クラス"""
    
    @staticmethod
    def calculate_sma(prices: pd.Series, window: int) -> pd.Series:
        """単純移動平均"""
        return prices.rolling(window=window).mean()
    
    @staticmethod
    def calculate_ema(prices: pd.Series, window: int) -> pd.Series:
        """指数移動平均"""
        return prices.ewm(span=window).mean()
    
    @staticmethod
    def calculate_rsi(prices: pd.Series, window: int = 14) -> pd.Series:
        """RSI（相対力指数）"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))
    
    @staticmethod
    def calculate_macd(prices: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9) -> pd.DataFrame:
        """MACD"""
        ema_fast = prices.ewm(span=fast).mean()
        ema_slow = prices.ewm(span=slow).mean()
        macd = ema_fast - ema_slow
        signal_line = macd.ewm(span=signal).mean()
        histogram = macd - signal_line
        
        return pd.DataFrame({
            'macd': macd,
            'signal': signal_line,
            'histogram': histogram
        })
    
    @staticmethod
    def calculate_bollinger_bands(prices: pd.Series, window: int = 20, std_dev: float = 2) -> pd.DataFrame:
        """ボリンジャーバンド"""
        sma = prices.rolling(window=window).mean()
        std = prices.rolling(window=window).std()
        
        return pd.DataFrame({
            'upper': sma + (std * std_dev),
            'middle': sma,
            'lower': sma - (std * std_dev)
        })
```

#### 収益率計算
```python
def calculate_returns(prices: pd.Series, method: str = 'log') -> pd.Series:
    """収益率の計算"""
    if method == 'log':
        return np.log(prices / prices.shift(1))
    elif method == 'simple':
        return prices.pct_change()
    elif method == 'forward':
        return prices.shift(-1) / prices - 1
    else:
        raise ValueError(f"未対応の収益率計算方法: {method}")
```

### 3. 特徴量統合

#### クロスセクショナル特徴量
```python
def create_cross_sectional_features(df: pd.DataFrame) -> pd.DataFrame:
    """クロスセクショナル特徴量の作成"""
    features = df.copy()
    
    # 業界内偏差値
    features['sector_zscore'] = features.groupby('sector')['return'].transform(
        lambda x: (x - x.mean()) / x.std()
    )
    
    # 市場内ランキング
    features['market_rank'] = features['return'].rank(pct=True)
    
    # 規模別ランキング
    features['size_rank'] = features.groupby('size_category')['return'].rank(pct=True)
    
    return features
```

#### 時系列特徴量
```python
def create_time_series_features(df: pd.DataFrame) -> pd.DataFrame:
    """時系列特徴量の作成"""
    features = df.copy()
    
    # ラグ特徴量
    for lag in [1, 2, 3, 5, 10, 20]:
        features[f'return_lag_{lag}'] = features['return'].shift(lag)
    
    # 移動平均
    for window in [5, 10, 20, 60]:
        features[f'ma_{window}'] = features['return'].rolling(window).mean()
    
    # ボラティリティ
    for window in [5, 10, 20]:
        features[f'vol_{window}'] = features['return'].rolling(window).std()
    
    # モメンタム
    for window in [5, 10, 20]:
        features[f'momentum_{window}'] = features['close'] / features['close'].shift(window) - 1
    
    return features
```

## データ品質管理

### 1. データ検証

#### 基本検証
```python
class DataValidator:
    """データ品質検証クラス"""
    
    def __init__(self, rules: Dict):
        self.rules = rules
    
    def validate_ohlcv(self, df: pd.DataFrame) -> Dict:
        """OHLCV データの検証"""
        issues = []
        
        # 基本チェック
        if df.empty:
            issues.append("データが空です")
        
        # 価格関係チェック
        if (df['high'] < df['low']).any():
            issues.append("高値が安値より低い箇所があります")
        
        if (df['close'] > df['high']).any() or (df['close'] < df['low']).any():
            issues.append("終値が高値・安値の範囲外です")
        
        # 異常値チェック
        price_change = df['close'].pct_change().abs()
        if (price_change > 0.3).any():  # 30%以上の変動
            issues.append("異常な価格変動が検出されました")
        
        # 欠損値チェック
        missing_ratio = df.isnull().sum() / len(df)
        if (missing_ratio > 0.05).any():  # 5%以上の欠損
            issues.append("欠損値が多すぎます")
        
        return {
            'is_valid': len(issues) == 0,
            'issues': issues,
            'quality_score': self._calculate_quality_score(df)
        }
    
    def _calculate_quality_score(self, df: pd.DataFrame) -> float:
        """品質スコアの計算"""
        score = 100.0
        
        # 欠損値ペナルティ
        missing_ratio = df.isnull().sum().sum() / (len(df) * len(df.columns))
        score -= missing_ratio * 50
        
        # 異常値ペナルティ
        for col in df.select_dtypes(include=[np.number]).columns:
            z_scores = np.abs(stats.zscore(df[col].dropna()))
            outlier_ratio = (z_scores > 3).sum() / len(z_scores)
            score -= outlier_ratio * 20
        
        return max(0, score)
```

#### 整合性チェック
```python
def check_data_consistency(df: pd.DataFrame) -> Dict:
    """データの整合性チェック"""
    issues = []
    
    # 時系列整合性
    if not df.index.is_monotonic_increasing:
        issues.append("時系列が昇順になっていません")
    
    # 営業日チェック
    business_days = pd.bdate_range(start=df.index.min(), end=df.index.max())
    missing_days = set(business_days) - set(df.index)
    if len(missing_days) > len(business_days) * 0.05:  # 5%以上欠損
        issues.append("営業日のデータが不足しています")
    
    # 価格連続性チェック
    price_gaps = df['close'].diff().abs() / df['close'].shift(1)
    if (price_gaps > 0.2).any():  # 20%以上のギャップ
        issues.append("価格に大きなギャップがあります")
    
    return {
        'is_consistent': len(issues) == 0,
        'issues': issues
    }
```

### 2. データ監視

#### リアルタイム監視
```python
class DataMonitor:
    """データ監視クラス"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.alerts = []
    
    def monitor_data_quality(self, df: pd.DataFrame, symbol: str) -> None:
        """データ品質の監視"""
        # 最新データのチェック
        latest_date = df.index.max()
        if (pd.Timestamp.now() - latest_date).days > 1:
            self._send_alert(f"{symbol}: データが古すぎます ({latest_date})")
        
        # 異常値検出
        recent_data = df.tail(5)
        if self._detect_anomalies(recent_data):
            self._send_alert(f"{symbol}: 異常値を検出しました")
        
        # ボリューム異常
        avg_volume = df['volume'].tail(20).mean()
        latest_volume = df['volume'].iloc[-1]
        if latest_volume > avg_volume * 10:
            self._send_alert(f"{symbol}: 異常な出来高です")
    
    def _detect_anomalies(self, df: pd.DataFrame) -> bool:
        """異常値検出"""
        # 価格変動の異常検出
        returns = df['close'].pct_change().abs()
        return (returns > 0.15).any()  # 15%以上の変動
    
    def _send_alert(self, message: str) -> None:
        """アラートの送信"""
        self.alerts.append({
            'timestamp': pd.Timestamp.now(),
            'message': message,
            'level': 'warning'
        })
        
        # 実際のアラート送信（メール、Slack等）
        print(f"[ALERT] {message}")
```

## データ保存・管理

### 1. 保存システム

#### 階層化保存
```python
class DataStorage:
    """データ保存管理クラス"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.raw_path = Path(config['raw_path'])
        self.processed_path = Path(config['processed_path'])
        self.feature_path = Path(config['feature_path'])
    
    def save_raw_data(self, symbol: str, df: pd.DataFrame) -> None:
        """生データの保存"""
        file_path = self.raw_path / f"{symbol}.csv"
        df.to_csv(file_path, index=True)
        
        # メタデータの保存
        metadata = {
            'symbol': symbol,
            'rows': len(df),
            'columns': list(df.columns),
            'date_range': [str(df.index.min()), str(df.index.max())],
            'saved_at': str(pd.Timestamp.now())
        }
        
        metadata_path = self.raw_path / f"{symbol}_metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
    
    def save_processed_data(self, symbol: str, df: pd.DataFrame) -> None:
        """加工データの保存"""
        file_path = self.processed_path / f"{symbol}_processed.parquet"
        df.to_parquet(file_path, index=True)
    
    def save_features(self, symbol: str, features: pd.DataFrame) -> None:
        """特徴量の保存"""
        file_path = self.feature_path / f"{symbol}_features.parquet"
        features.to_parquet(file_path, index=True)
```

#### バックアップ・復元
```python
def backup_data(source_path: str, backup_path: str) -> None:
    """データのバックアップ"""
    timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')
    backup_dir = Path(backup_path) / f"backup_{timestamp}"
    
    shutil.copytree(source_path, backup_dir)
    
    # 圧縮保存
    shutil.make_archive(str(backup_dir), 'zip', backup_dir)
    shutil.rmtree(backup_dir)

def restore_data(backup_file: str, restore_path: str) -> None:
    """データの復元"""
    with zipfile.ZipFile(backup_file, 'r') as zip_ref:
        zip_ref.extractall(restore_path)
```

### 2. データベース設計

#### テーブル設計
```sql
-- 株価データテーブル
CREATE TABLE stock_prices (
    id SERIAL PRIMARY KEY,
    symbol VARCHAR(10) NOT NULL,
    date DATE NOT NULL,
    open DECIMAL(10,2),
    high DECIMAL(10,2),
    low DECIMAL(10,2),
    close DECIMAL(10,2),
    volume BIGINT,
    adjusted_close DECIMAL(10,2),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(symbol, date)
);

-- 特徴量テーブル
CREATE TABLE features (
    id SERIAL PRIMARY KEY,
    symbol VARCHAR(10) NOT NULL,
    date DATE NOT NULL,
    feature_name VARCHAR(50) NOT NULL,
    feature_value DECIMAL(15,8),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(symbol, date, feature_name)
);

-- データ品質テーブル
CREATE TABLE data_quality (
    id SERIAL PRIMARY KEY,
    symbol VARCHAR(10) NOT NULL,
    date DATE NOT NULL,
    quality_score DECIMAL(5,2),
    issues TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### インデックス設計
```sql
-- パフォーマンス向上のためのインデックス
CREATE INDEX idx_stock_prices_symbol_date ON stock_prices(symbol, date);
CREATE INDEX idx_features_symbol_date ON features(symbol, date);
CREATE INDEX idx_data_quality_symbol_date ON data_quality(symbol, date);
```

## パフォーマンス最適化

### 1. 並列処理

#### 多銘柄並列処理
```python
import concurrent.futures
import asyncio

async def process_multiple_symbols(symbols: List[str]) -> Dict:
    """複数銘柄の並列処理"""
    results = {}
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        futures = {
            executor.submit(process_single_symbol, symbol): symbol 
            for symbol in symbols
        }
        
        for future in concurrent.futures.as_completed(futures):
            symbol = futures[future]
            try:
                result = future.result()
                results[symbol] = result
            except Exception as e:
                print(f"Error processing {symbol}: {e}")
                results[symbol] = None
    
    return results
```

### 2. キャッシュ戦略

#### メモリキャッシュ
```python
from functools import lru_cache
import redis

class DataCache:
    """データキャッシュクラス"""
    
    def __init__(self, redis_config: Dict):
        self.redis_client = redis.Redis(**redis_config)
        self.cache_ttl = 3600  # 1時間
    
    @lru_cache(maxsize=1000)
    def get_processed_data(self, symbol: str, date: str) -> pd.DataFrame:
        """処理済みデータの取得（メモリキャッシュ）"""
        cache_key = f"processed:{symbol}:{date}"
        
        # Redisキャッシュを確認
        cached_data = self.redis_client.get(cache_key)
        if cached_data:
            return pd.read_json(cached_data)
        
        # データベースから取得
        data = self._load_from_db(symbol, date)
        
        # キャッシュに保存
        self.redis_client.setex(
            cache_key, 
            self.cache_ttl, 
            data.to_json()
        )
        
        return data
```

## 使用例

### 基本的な使用方法
```python
# データ収集システムの初期化
collector = DataCollector(config)

# 日本株データの収集
symbols = ['7203.T', '9984.T', '6758.T']
results = await collector.collect_stock_data(
    symbols=symbols,
    start_date='2023-01-01',
    end_date='2024-06-30'
)

# 特徴量の生成
feature_engineer = FeatureEngineer()
for symbol in symbols:
    raw_data = load_raw_data(symbol)
    features = feature_engineer.create_features(raw_data)
    save_features(symbol, features)
```

### 高度な使用例
```python
# 品質監視付きデータ処理
monitor = DataMonitor(config)
validator = DataValidator(validation_rules)

for symbol in symbols:
    data = collect_data(symbol)
    
    # 品質検証
    validation_result = validator.validate_ohlcv(data)
    if not validation_result['is_valid']:
        print(f"品質問題: {validation_result['issues']}")
    
    # 監視
    monitor.monitor_data_quality(data, symbol)
    
    # 処理・保存
    processed_data = process_data(data)
    save_data(symbol, processed_data)
```

このデータ処理システムにより、高品質で一貫性のある金融データを効率的に管理することができます。